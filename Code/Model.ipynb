{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0835f5c-198e-454a-8f15-8d6a79a68a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import feature_extraction, linear_model, model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26679e4-8055-4ebb-8c20-57cc65c81de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Imports:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics as st\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import regex\n",
    "import gc\n",
    "import re\n",
    "import string\n",
    "import operator\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import tokenization\n",
    "from wordcloud import STOPWORDS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#  Metrics Import:\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c38f3331-8e90-469d-9da2-dd73f560e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in Datasets:\n",
    "df_train_cleanedMislabelsDuplicates = pd.read_csv(\"../Data/df_train_cleanedMislabelsDuplicates.csv\", encoding='utf-8')\n",
    "df_train_cleanedNoMislabelsDuplicates = pd.read_csv(\"../Data/df_train_cleanedNoMislabelsDuplicates.csv\", encoding='utf-8')\n",
    "df_train_cleanedMislabelsNoDuplicates = pd.read_csv(\"../Data/df_train_cleanedMislabelsNoDuplicates.csv\", encoding='utf-8')\n",
    "df_train_cleanedNoMislabelsNoDuplicates = pd.read_csv(\"../Data/df_train_cleanedNoMislabelsNoDuplicates.csv\", encoding='utf-8')\n",
    "df_test_cleaned = pd.read_csv(\"../Data/df_test_cleaned.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f0570c-0e51-45af-a54c-b0d3edffc57a",
   "metadata": {},
   "source": [
    "#### Building Vectors:\n",
    "We use sckit-learn's CountVectorizer to count the words in each tweet and turn them into data our machine learning model can process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f675506b-bd3f-41c9-8036-e25509efb83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "\n",
    "train_vectors = count_vectorizer.fit_transform(df_train_cleanedNoMislabelsNoDuplicates['text'])\n",
    "\n",
    "#For test_vectors, we only use .transform() function b/c we need that the train and test vectors use the same set of tokens\n",
    "test_vectors = count_vectorizer.transform(df_test_cleaned[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0256181b-fff7-48db-a63e-f6079a57c6b2",
   "metadata": {},
   "source": [
    "#### Building Model:\n",
    "We use the words contained in each tweet as the predictor variable for real/fake disaster tweet (1/0). We assume a linear model/connection in this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4549f19-a372-4e8c-a764-61e0a4b960c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [0.5767098  0.53946621 0.5949214 ]\n"
     ]
    }
   ],
   "source": [
    "# Since the vectors are big, we want to push the model's weights toward 0 without completely discounting different words\n",
    "# Thus, we use Ridge Regression\n",
    "clf = linear_model.RidgeClassifier()\n",
    "# cv = 3 means we are using Three-Fold Cross Validation\n",
    "scores = model_selection.cross_val_score(clf, train_vectors, df_train_cleanedNoMislabelsNoDuplicates['target_relabeled'], cv = 3, scoring = \"f1\")\n",
    "print(\"Scores: \", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32e33e1-8576-4ad0-8e39-84e2e0ce3146",
   "metadata": {},
   "source": [
    "Ways to improve this score: Do TFIDF, LSA, LSTM/RNNs, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623320ee-30ed-47a4-a245-58ef7f6b3f9c",
   "metadata": {},
   "source": [
    "#### Predictions and Submission File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dc04fd8-a4c1-4d3c-9729-2ae95a548153",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(train_vectors, train_df['target'])\n",
    "tutorial_submission = pd.read_csv(\"../Data/sample_submission.csv\")\n",
    "tutorial_submission['target'] = clf.predict(test_vectors)\n",
    "tutorial_submission.to_csv(\"../Result Files/submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50db6d7a-e2c3-4bb6-bbd4-38b6f0e95410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
