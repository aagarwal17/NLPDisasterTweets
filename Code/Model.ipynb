{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39f0b300-670f-42cf-8fed-1d21b9f212d6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.1.1 in c:\\users\\aruna\\anaconda3\\lib\\site-packages (4.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from transformers==4.1.1) (22.0)\n",
      "Requirement already satisfied: requests in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from transformers==4.1.1) (2.21.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from transformers==4.1.1) (0.0.53)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from transformers==4.1.1) (0.9.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from transformers==4.1.1) (4.64.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from transformers==4.1.1) (2022.7.9)\n",
      "Requirement already satisfied: numpy in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from transformers==4.1.1) (1.23.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from transformers==4.1.1) (3.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.1.1) (0.4.6)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from requests->transformers==4.1.1) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from requests->transformers==4.1.1) (2022.12.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from requests->transformers==4.1.1) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from requests->transformers==4.1.1) (1.24.3)\n",
      "Requirement already satisfied: click in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from sacremoses->transformers==4.1.1) (8.0.4)\n",
      "Requirement already satisfied: six in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from sacremoses->transformers==4.1.1) (1.12.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from sacremoses->transformers==4.1.1) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install transformers==4.1.1 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b9b2462-d229-42c0-a321-9816aafe7fbd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "simpletransformers 0.63.9 requires transformers>=4.6.0, but you have transformers 4.1.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.1.1\n",
      "  Using cached transformers-4.1.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.53-py3-none-any.whl\n",
      "Requirement already satisfied: tokenizers==0.9.4 in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from transformers==4.1.1) (0.9.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from transformers==4.1.1) (3.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from transformers==4.1.1) (2022.7.9)\n",
      "Requirement already satisfied: numpy in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from transformers==4.1.1) (1.23.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from transformers==4.1.1) (22.0)\n",
      "Requirement already satisfied: requests in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from transformers==4.1.1) (2.21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from transformers==4.1.1) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.1.1) (0.4.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from requests->transformers==4.1.1) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from requests->transformers==4.1.1) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from requests->transformers==4.1.1) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from requests->transformers==4.1.1) (1.24.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from sacremoses->transformers==4.1.1) (1.1.1)\n",
      "Requirement already satisfied: click in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from sacremoses->transformers==4.1.1) (8.0.4)\n",
      "Requirement already satisfied: six in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from sacremoses->transformers==4.1.1) (1.12.0)\n",
      "Installing collected packages: sacremoses, transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.27.1\n",
      "    Uninstalling transformers-4.27.1:\n",
      "      Successfully uninstalled transformers-4.27.1\n",
      "Successfully installed sacremoses-0.0.53 transformers-4.1.1\n",
      "Requirement already satisfied: sentencepiece==0.1.94 in c:\\users\\aruna\\anaconda3\\lib\\site-packages (0.1.94)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ransformers (c:\\users\\aruna\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\users\\aruna\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\users\\aruna\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\users\\aruna\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\users\\aruna\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\users\\aruna\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mosestokenizer==1.1.0\n",
      "  Downloading mosestokenizer-1.1.0.tar.gz (37 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting docopt\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting openfile\n",
      "  Downloading openfile-0.0.7-py3-none-any.whl (2.4 kB)\n",
      "Collecting uctools\n",
      "  Downloading uctools-1.3.0.tar.gz (4.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting toolwrapper\n",
      "  Downloading toolwrapper-2.1.0.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: mosestokenizer, docopt, toolwrapper, uctools\n",
      "  Building wheel for mosestokenizer (setup.py): started\n",
      "  Building wheel for mosestokenizer (setup.py): finished with status 'done'\n",
      "  Created wheel for mosestokenizer: filename=mosestokenizer-1.1.0-py3-none-any.whl size=49119 sha256=d68063149e2017a3a0309fa5af8ed1b77c2a3978082e123197e2384def63ebbc\n",
      "  Stored in directory: c:\\users\\aruna\\appdata\\local\\pip\\cache\\wheels\\79\\0d\\d1\\36adc4fa4953636f9f304c82add5d25be321ff21365d8e47dc\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=a6036f44c12358e903ec18d387240f72bb5c8dcb3fe4a6de4f35a55ff610ccb7\n",
      "  Stored in directory: c:\\users\\aruna\\appdata\\local\\pip\\cache\\wheels\\ca\\cc\\e3\\f1e272f628fdb013d969acc99cfe2e031ea15b3efb74ffe842\n",
      "  Building wheel for toolwrapper (setup.py): started\n",
      "  Building wheel for toolwrapper (setup.py): finished with status 'done'\n",
      "  Created wheel for toolwrapper: filename=toolwrapper-2.1.0-py3-none-any.whl size=3347 sha256=dc2c31ef79aa0553ffa33d6bd15ec4e6db411d4c6b0c816f437d9f4671b16ce8\n",
      "  Stored in directory: c:\\users\\aruna\\appdata\\local\\pip\\cache\\wheels\\75\\3b\\c4\\62791e6666131c98adba13094857a62bcc776baa549524eafb\n",
      "  Building wheel for uctools (setup.py): started\n",
      "  Building wheel for uctools (setup.py): finished with status 'done'\n",
      "  Created wheel for uctools: filename=uctools-1.3.0-py3-none-any.whl size=6162 sha256=0c7ce933e95bb4e0900ebbd5b5f5d0f77ccaf10b685d0c7cf3777a5d090344b5\n",
      "  Stored in directory: c:\\users\\aruna\\appdata\\local\\pip\\cache\\wheels\\cf\\2f\\d9\\9425df6b6f5a0ef152af806344debd9ad5afd283a73c8f07cd\n",
      "Successfully built mosestokenizer docopt toolwrapper uctools\n",
      "Installing collected packages: toolwrapper, openfile, docopt, uctools, mosestokenizer\n",
      "Successfully installed docopt-0.6.2 mosestokenizer-1.1.0 openfile-0.0.7 toolwrapper-2.1.0 uctools-1.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.1.1\n",
    "!pip install sentencepiece==0.1.94\n",
    "!pip install mosestokenizer==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0835f5c-198e-454a-8f15-8d6a79a68a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "import logging\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import feature_extraction, linear_model, model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82604a7f-cdeb-476b-950c-1d52652f4b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26679e4-8055-4ebb-8c20-57cc65c81de5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Library Imports:\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import statistics as st\n",
    "# from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import regex\n",
    "# import gc\n",
    "# import re\n",
    "# import string\n",
    "# import operator\n",
    "# from collections import defaultdict\n",
    "# import sys\n",
    "# import tokenization\n",
    "# from wordcloud import STOPWORDS\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# #  Metrics Import:\n",
    "# from sklearn.metrics import classification_report \n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import f1_score\n",
    "# from sklearn.metrics import recall_score\n",
    "# from sklearn.metrics import precision_score\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c38f3331-8e90-469d-9da2-dd73f560e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in Datasets:\n",
    "df_train_cleanedMislabelsDuplicates = pd.read_csv(\"../Data/df_train_cleanedMislabelsDuplicates.csv\", encoding='utf-8')\n",
    "df_train_cleanedNoMislabelsDuplicates = pd.read_csv(\"../Data/df_train_cleanedNoMislabelsDuplicates.csv\", encoding='utf-8')\n",
    "df_train_cleanedMislabelsNoDuplicates = pd.read_csv(\"../Data/df_train_cleanedMislabelsNoDuplicates.csv\", encoding='utf-8')\n",
    "df_train_cleanedNoMislabelsNoDuplicates = pd.read_csv(\"../Data/df_train_cleanedNoMislabelsNoDuplicates.csv\", encoding='utf-8')\n",
    "df_test_cleaned = pd.read_csv(\"../Data/df_test_cleaned.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09a7bc57-7244-448c-961e-80d5ea0988b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target_relabeled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>no_location</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>no_location</td>\n",
       "      <td>forest fire near los angeles ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>no_location</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>no_location</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>no_location</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6882</th>\n",
       "      <td>10859</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>no_location</td>\n",
       "      <td>breaking los angeles refugio oil spill may hav...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6883</th>\n",
       "      <td>10860</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>no_location</td>\n",
       "      <td>a siren just went off and it was not the forne...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6884</th>\n",
       "      <td>10862</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>no_location</td>\n",
       "      <td>officials say a quarantine is in place at an a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6885</th>\n",
       "      <td>10864</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>no_location</td>\n",
       "      <td>on the flip side i am at walmart and there is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6886</th>\n",
       "      <td>10866</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>no_location</td>\n",
       "      <td>suicide bomber kills 15 in saudi security site...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6887 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id     keyword     location  \\\n",
       "0         1  no_keyword  no_location   \n",
       "1         4  no_keyword  no_location   \n",
       "2         5  no_keyword  no_location   \n",
       "3         6  no_keyword  no_location   \n",
       "4         7  no_keyword  no_location   \n",
       "...     ...         ...          ...   \n",
       "6882  10859  no_keyword  no_location   \n",
       "6883  10860  no_keyword  no_location   \n",
       "6884  10862  no_keyword  no_location   \n",
       "6885  10864  no_keyword  no_location   \n",
       "6886  10866  no_keyword  no_location   \n",
       "\n",
       "                                                   text  target_relabeled  \n",
       "0     our deeds are the reason of this earthquake ma...                 1  \n",
       "1        forest fire near los angeles ronge sask canada                 1  \n",
       "2     all residents asked to shelter in place are be...                 1  \n",
       "3     13000 people receive wildfires evacuation orde...                 1  \n",
       "4     just got sent this photo from ruby alaska as s...                 1  \n",
       "...                                                 ...               ...  \n",
       "6882  breaking los angeles refugio oil spill may hav...                 1  \n",
       "6883  a siren just went off and it was not the forne...                 1  \n",
       "6884  officials say a quarantine is in place at an a...                 1  \n",
       "6885  on the flip side i am at walmart and there is ...                 1  \n",
       "6886  suicide bomber kills 15 in saudi security site...                 1  \n",
       "\n",
       "[6887 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_cleanedNoMislabelsNoDuplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff41e6a5-2387-4578-a410-f12bc562fc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_train_cleanedMislabelsDuplicates, df_train_cleanedNoMislabelsDuplicates, df_train_cleanedMislabelsNoDuplicates, df_train_cleanedNoMislabelsNoDuplicates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b2141c9-b495-4b66-84a3-da0963a2f6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cleanedMislabelsNoDuplicates.drop(['location', 'id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01361bf5-9086-4b9b-a3e9-460138796487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try changing tokenizer_class to processor_class or decreasing python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b0fcdc7-9b92-4445-93b3-c79ade3b3ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.marian.modeling_marian because of the following error (look up to see its traceback):\nadd_code_sample_docstrings() got an unexpected keyword argument 'tokenizer_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py:1126\u001b[0m, in \u001b[0;36m_get_module\u001b[1;34m(self, module_name)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:783\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"PyTorch MarianMTModel model, ported from the Marian C++ repo.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbart\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_bart\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BartForConditionalGeneration\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_marian\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MarianConfig\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1027\u001b[0m\n\u001b[0;32m   1014\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPastAndCrossAttentions(\n\u001b[0;32m   1015\u001b[0m             last_hidden_state\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m   1016\u001b[0m             past_key_values\u001b[38;5;241m=\u001b[39mnext_cache,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1019\u001b[0m             cross_attentions\u001b[38;5;241m=\u001b[39mall_cross_attentions,\n\u001b[0;32m   1020\u001b[0m         )\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;129m@add_start_docstrings\u001b[39m(\n\u001b[0;32m   1024\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe bare BART Model outputting raw hidden-states without any specific head on top.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1025\u001b[0m     BART_START_DOCSTRING,\n\u001b[0;32m   1026\u001b[0m )\n\u001b[1;32m-> 1027\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBartModel\u001b[39;00m(BartPretrainedModel):\n\u001b[0;32m   1028\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: BartConfig):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1054\u001b[0m, in \u001b[0;36mBartModel\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(BART_INPUTS_DOCSTRING)\n\u001b[1;32m-> 1054\u001b[0m \u001b[38;5;129m@add_code_sample_docstrings\u001b[39m\u001b[43m(\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_TOKENIZER_FOR_DOC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/bart-large\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSeq2SeqModelOutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_CONFIG_FOR_DOC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m   1061\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1062\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1063\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1064\u001b[0m     decoder_input_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1065\u001b[0m     decoder_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1066\u001b[0m     encoder_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1067\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1068\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1069\u001b[0m     decoder_inputs_embeds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1070\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1071\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1072\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1073\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1074\u001b[0m ):\n\u001b[0;32m   1075\u001b[0m \n\u001b[0;32m   1076\u001b[0m     \u001b[38;5;66;03m# 4.12.20 (PVP): Not a fan of this \"magical\" function and\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;66;03m# also wonder how often it's actually used ... keep now\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[38;5;66;03m# for backward compatibility\u001b[39;00m\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;66;03m# -> is this used for backward compatibility\u001b[39;00m\n\u001b[0;32m   1080\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: add_code_sample_docstrings() got an unexpected keyword argument 'tokenizer_class'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MarianMTModel, MarianTokenizer\n\u001b[0;32m      2\u001b[0m target_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-en-ROMANCE\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m target_tokenizer \u001b[38;5;241m=\u001b[39m MarianTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(target_model_name)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1039\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py:1117\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py:1116\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py:1128\u001b[0m, in \u001b[0;36m_get_module\u001b[1;34m(self, module_name)\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.marian.modeling_marian because of the following error (look up to see its traceback):\nadd_code_sample_docstrings() got an unexpected keyword argument 'tokenizer_class'"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "target_model_name = 'Helsinki-NLP/opus-mt-en-ROMANCE'\n",
    "target_tokenizer = MarianTokenizer.from_pretrained(target_model_name)\n",
    "target_model = MarianMTModel.from_pretrained(target_model_name).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1762709-0d7f-4717-a187-fb75dd04f934",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "en_model_name = 'Helsinki-NLP/opus-mt-ROMANCE-en'\n",
    "en_tokenizer = MarianTokenizer.from_pretrained(en_model_name)\n",
    "en_model = MarianMTModel.from_pretrained(en_model_name).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51ac73c4-177a-4ccf-b166-a49cce535424",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def translate(texts, model, tokenizer, language=\"fr\"):\n",
    "    # Prepare the text data into appropriate format for the model\n",
    "    template = lambda text: f\"{text}\" if language == \"en\" else f\">>{language}<< {text}\"\n",
    "    src_texts = [template(text) for text in texts]\n",
    "\n",
    "    # Tokenize the texts\n",
    "    encoded = tokenizer.prepare_seq2seq_batch(src_texts,return_tensors=\"pt\").to('cuda:0')\n",
    "    \n",
    "    # Generate translation using model\n",
    "    translated = model.generate(**encoded)\n",
    "\n",
    "    # Convert the generated tokens indices back into text\n",
    "    translated_texts = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    \n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9f8bbfb-5b33-4abb-bfe6-e93f3bcf62f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def back_translate(texts, source_lang=\"en\", target_lang=\"fr\"):\n",
    "    # Translate from source to target language\n",
    "    fr_texts = translate(texts, target_model, target_tokenizer, \n",
    "                         language=target_lang)\n",
    "\n",
    "    # Translate from target language back to source language\n",
    "    back_translated_texts = translate(fr_texts, en_model, en_tokenizer, \n",
    "                                      language=source_lang)\n",
    "    \n",
    "    return back_translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7b96fbc-6dbf-4ff0-9161-aca400260bc6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m en_texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis is so cool\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI hated the food\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThey were very helpful\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m aug_texts \u001b[38;5;241m=\u001b[39m \u001b[43mback_translate\u001b[49m\u001b[43m(\u001b[49m\u001b[43men_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(aug_texts)\n",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m, in \u001b[0;36mback_translate\u001b[1;34m(texts, source_lang, target_lang)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mback_translate\u001b[39m(texts, source_lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m, target_lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Translate from source to target language\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     fr_texts \u001b[38;5;241m=\u001b[39m translate(texts, \u001b[43mtarget_model\u001b[49m, target_tokenizer, \n\u001b[0;32m      4\u001b[0m                          language\u001b[38;5;241m=\u001b[39mtarget_lang)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Translate from target language back to source language\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     back_translated_texts \u001b[38;5;241m=\u001b[39m translate(fr_texts, en_model, en_tokenizer, \n\u001b[0;32m      8\u001b[0m                                       language\u001b[38;5;241m=\u001b[39msource_lang)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'target_model' is not defined"
     ]
    }
   ],
   "source": [
    "en_texts = ['This is so cool', 'I hated the food', 'They were very helpful']\n",
    "aug_texts = back_translate(en_texts, source_lang=\"en\", target_lang=\"es\")\n",
    "print(aug_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ae5817-a067-49af-84c2-7cc93f63d989",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "translated_text = back_translate(list(df['text']), source_lang=\"en\", target_lang=\"fr\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "047b333e-0ea3-4eef-947c-e38ad9a77a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24ddca8f17c4dd5ae2a8863d297962b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dced4ac1c1c042bcb50f00b24f5714d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e03007813f41d5b2e5aed8c4021d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e6869afa5c477c908e52cc098cf9b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#specify simple transformer model with number of epochs\n",
    "model_args = ClassificationArgs(num_train_epochs = 4,learning_rate = 3e-6)\n",
    "model = ClassificationModel(\n",
    "    \"roberta\", \"roberta-base\", args=model_args, use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895fe2ab-5f51-4c04-94a6-bc10abe4563f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aruna\\anaconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:612: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5540ed7c2149089ef62e09674c58ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6887 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Train model and evaluate test set\n",
    "model.train_model(df_train_cleanedMislabelsNoDuplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658aed7a-5329-4d8b-856c-5a78f09a40e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Test DataFrame ready to be used for predictions:\n",
    "test.drop(['keyword', 'location', 'id'], axis=1, inplace=True)\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef989c33-687b-4903-8b56-622da72c0ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputing Predictions:\n",
    "predictions, raw_outputs = model.predict(list(test['text']))\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61afbec-cd93-4bcb-80ff-cd006f3521b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission['target'] = predictions\n",
    "submission.to_csv('submission.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff4c9d1-52aa-455c-a817-edfbd58d6c14",
   "metadata": {},
   "source": [
    "## 2nd Model Attempt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7908cf0-8cf8-4ffe-85c1-5f0ed34082a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4b1ab8a-e52c-4adc-966e-fdc09fcfc005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenization.py', <http.client.HTTPMessage at 0x28659a47040>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import urllib.request\n",
    "# url = 'https://github.com/tensorflow/models/blob/master/official/nlp/tools/tokenization.py'\n",
    "# filename = 'tokenization.py'\n",
    "# urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5649276e-8144-4cb7-9f8d-ef157f06bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import re\n",
    "# import string\n",
    "# import operator\n",
    "# from collections import defaultdict\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "import tokenization\n",
    "# from wordcloud import STOPWORDS\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "\n",
    "SEED = 1337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26781606-819e-4bd6-a0f4-82937550070d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole Training Set Shape = (6887, 3)\n",
      "Whole Training Set Unique keyword Count = 222\n",
      "Whole Training Set Target Rate (Disaster) 2812/4075 (Not Disaster)\n",
      "\n",
      "Fold 1 Training Set Shape = (3443,) - Validation Set Shape = (3444,)\n",
      "Fold 1 Training Set Unique keyword Count = 222 - Validation Set Unique keyword Count = 222\n",
      "\n",
      "Fold 2 Training Set Shape = (3444,) - Validation Set Shape = (3443,)\n",
      "Fold 2 Training Set Unique keyword Count = 222 - Validation Set Unique keyword Count = 222\n"
     ]
    }
   ],
   "source": [
    "K = 2\n",
    "skf = StratifiedKFold(n_splits=K, random_state=SEED, shuffle=True)\n",
    "\n",
    "DISASTER = df_train_cleanedMislabelsNoDuplicates['target'] == 1\n",
    "print('Whole Training Set Shape = {}'.format(df_train_cleanedMislabelsNoDuplicates.shape))\n",
    "print('Whole Training Set Unique keyword Count = {}'.format(df_train_cleanedMislabelsNoDuplicates['keyword'].nunique()))\n",
    "print('Whole Training Set Target Rate (Disaster) {}/{} (Not Disaster)'.format(df_train_cleanedMislabelsNoDuplicates[DISASTER]['target'].count(), df_train_cleanedMislabelsNoDuplicates[~DISASTER]['target'].count()))\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(df_train_cleanedMislabelsNoDuplicates['text'], df_train_cleanedMislabelsNoDuplicates['target']), 1):\n",
    "    print('\\nFold {} Training Set Shape = {} - Validation Set Shape = {}'.format(fold, df_train_cleanedMislabelsNoDuplicates.loc[trn_idx, 'text'].shape, df_train_cleanedMislabelsNoDuplicates.loc[val_idx, 'text'].shape))\n",
    "    print('Fold {} Training Set Unique keyword Count = {} - Validation Set Unique keyword Count = {}'.format(fold, df_train_cleanedMislabelsNoDuplicates.loc[trn_idx, 'keyword'].nunique(), df_train_cleanedMislabelsNoDuplicates.loc[val_idx, 'keyword'].nunique()))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "864adebd-ba9b-44b7-a1b6-db67502ca7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationReport(Callback):\n",
    "    \n",
    "    def __init__(self, train_data=(), validation_data=()):\n",
    "        super(Callback, self).__init__()\n",
    "        \n",
    "        self.X_train, self.y_train = train_data\n",
    "        self.train_precision_scores = []\n",
    "        self.train_recall_scores = []\n",
    "        self.train_f1_scores = []\n",
    "        \n",
    "        self.X_val, self.y_val = validation_data\n",
    "        self.val_precision_scores = []\n",
    "        self.val_recall_scores = []\n",
    "        self.val_f1_scores = [] \n",
    "               \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        train_predictions = np.round(self.model.predict(self.X_train, verbose=0))        \n",
    "        train_precision = precision_score(self.y_train, train_predictions, average='macro')\n",
    "        train_recall = recall_score(self.y_train, train_predictions, average='macro')\n",
    "        train_f1 = f1_score(self.y_train, train_predictions, average='macro')\n",
    "        self.train_precision_scores.append(train_precision)        \n",
    "        self.train_recall_scores.append(train_recall)\n",
    "        self.train_f1_scores.append(train_f1)\n",
    "        \n",
    "        val_predictions = np.round(self.model.predict(self.X_val, verbose=0))\n",
    "        val_precision = precision_score(self.y_val, val_predictions, average='macro')\n",
    "        val_recall = recall_score(self.y_val, val_predictions, average='macro')\n",
    "        val_f1 = f1_score(self.y_val, val_predictions, average='macro')\n",
    "        self.val_precision_scores.append(val_precision)        \n",
    "        self.val_recall_scores.append(val_recall)        \n",
    "        self.val_f1_scores.append(val_f1)\n",
    "        \n",
    "        print('\\nEpoch: {} - Training Precision: {:.6} - Training Recall: {:.6} - Training F1: {:.6}'.format(epoch + 1, train_precision, train_recall, train_f1))\n",
    "        print('Epoch: {} - Validation Precision: {:.6} - Validation Recall: {:.6} - Validation F1: {:.6}'.format(epoch + 1, val_precision, val_recall, val_f1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d3ba991-1a4f-4fa5-a2d5-740f9a95d12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n",
      "CPU times: total: 6.16 s\n",
      "Wall time: 14.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1', trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce475c49-5df3-4dfd-9518-aacbc69ae307",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisasterDetector:\n",
    "    \n",
    "    def __init__(self, bert_layer, max_seq_length=128, lr=0.0001, epochs=15, batch_size=32):\n",
    "        \n",
    "        # BERT and Tokenization params\n",
    "        self.bert_layer = bert_layer\n",
    "        \n",
    "        self.max_seq_length = max_seq_length        \n",
    "        vocab_file = self.bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "        do_lower_case = self.bert_layer.resolved_object.do_lower_case.numpy()\n",
    "        self.tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "        \n",
    "        # Learning control params\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.models = []\n",
    "        self.scores = {}\n",
    "        \n",
    "    def encode(self, texts):\n",
    "                \n",
    "        all_tokens = []\n",
    "        all_masks = []\n",
    "        all_segments = []\n",
    "\n",
    "        for text in texts:\n",
    "            text = self.tokenizer.tokenize(text)\n",
    "            text = text[:self.max_seq_length - 2]\n",
    "            input_sequence = ['[CLS]'] + text + ['[SEP]']\n",
    "            pad_len = self.max_seq_length - len(input_sequence)\n",
    "\n",
    "            tokens = self.tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "            tokens += [0] * pad_len\n",
    "            pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "            segment_ids = [0] * self.max_seq_length\n",
    "\n",
    "            all_tokens.append(tokens)\n",
    "            all_masks.append(pad_masks)\n",
    "            all_segments.append(segment_ids)\n",
    "\n",
    "        return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
    "    def build_model(self):\n",
    "        \n",
    "        input_word_ids = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='input_word_ids')\n",
    "        input_mask = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='input_mask')\n",
    "        segment_ids = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='segment_ids')    \n",
    "        \n",
    "        pooled_output, sequence_output = self.bert_layer([input_word_ids, input_mask, segment_ids])   \n",
    "        clf_output = sequence_output[:, 0, :]\n",
    "        out = Dense(1, activation='sigmoid')(clf_output)\n",
    "        \n",
    "        model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "        optimizer = SGD(learning_rate=self.lr, momentum=0.8)\n",
    "        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train(self, X):\n",
    "\n",
    "        for fold, (trn_idx, val_idx) in enumerate(skf.split(X['text'], X['keyword'])):\n",
    "            print('\\nFold {}\\n'.format(fold))\n",
    "            X_trn_encoded = self.encode(X.loc[trn_idx, 'text'].str.lower())\n",
    "            y_trn = X.loc[trn_idx, 'target']\n",
    "            X_val_encoded = self.encode(X.loc[val_idx, 'text'].str.lower())\n",
    "            y_val = X.loc[val_idx, 'target']\n",
    "        \n",
    "            # Callbacks\n",
    "            metrics = ClassificationReport(train_data=(X_trn_encoded, y_trn), validation_data=(X_val_encoded, y_val))\n",
    "            \n",
    "            # Model\n",
    "            model = self.build_model()        \n",
    "            model.fit(X_trn_encoded, y_trn, validation_data=(X_val_encoded, y_val), callbacks=[metrics], epochs=self.epochs, batch_size=self.batch_size)\n",
    "            \n",
    "            self.models.append(model)\n",
    "            self.scores[fold] = {\n",
    "                'train': {\n",
    "                    'precision': metrics.train_precision_scores,\n",
    "                    'recall': metrics.train_recall_scores,\n",
    "                    'f1': metrics.train_f1_scores                    \n",
    "                },\n",
    "                'validation': {\n",
    "                    'precision': metrics.val_precision_scores,\n",
    "                    'recall': metrics.val_recall_scores,\n",
    "                    'f1': metrics.val_f1_scores                    \n",
    "                }\n",
    "            }\n",
    "                    \n",
    "                \n",
    "    def plot_learning_curve(self):\n",
    "        \n",
    "        fig, axes = plt.subplots(nrows=K, ncols=2, figsize=(20, K * 6), dpi=100)\n",
    "    \n",
    "        for i in range(K):\n",
    "            \n",
    "            # Classification Report curve\n",
    "            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[i].history.history['val_accuracy'], ax=axes[i][0], label='val_accuracy')\n",
    "            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['precision'], ax=axes[i][0], label='val_precision')\n",
    "            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['recall'], ax=axes[i][0], label='val_recall')\n",
    "            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['f1'], ax=axes[i][0], label='val_f1')        \n",
    "\n",
    "            axes[i][0].legend() \n",
    "            axes[i][0].set_title('Fold {} Validation Classification Report'.format(i), fontsize=14)\n",
    "\n",
    "            # Loss curve\n",
    "            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[0].history.history['loss'], ax=axes[i][1], label='train_loss')\n",
    "            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[0].history.history['val_loss'], ax=axes[i][1], label='val_loss')\n",
    "\n",
    "            axes[i][1].legend() \n",
    "            axes[i][1].set_title('Fold {} Train / Validation Loss'.format(i), fontsize=14)\n",
    "\n",
    "            for j in range(2):\n",
    "                axes[i][j].set_xlabel('Epoch', size=12)\n",
    "                axes[i][j].tick_params(axis='x', labelsize=12)\n",
    "                axes[i][j].tick_params(axis='y', labelsize=12)\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        X_test_encoded = self.encode(X['text'].str.lower())\n",
    "        y_pred = np.zeros((X_test_encoded[0].shape[0], 1))\n",
    "        for model in self.models:\n",
    "            y_pred += model.predict(X_test_encoded) / len(self.models)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6edcb5-0583-434b-a84e-50a092ea9685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0\n",
      "\n",
      "Epoch 1/10\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.6337 - accuracy: 0.6520 \n",
      "Epoch: 1 - Training Precision: 0.735271 - Training Recall: 0.696455 - Training F1: 0.701831\n",
      "Epoch: 1 - Validation Precision: 0.719405 - Validation Recall: 0.683083 - Validation F1: 0.686151\n",
      "108/108 [==============================] - 9695s 90s/step - loss: 0.6337 - accuracy: 0.6520 - val_loss: 0.5972 - val_accuracy: 0.7152\n",
      "Epoch 2/10\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.5509 - accuracy: 0.7557 \n",
      "Epoch: 2 - Training Precision: 0.772535 - Training Recall: 0.747783 - Training F1: 0.754368\n",
      "Epoch: 2 - Validation Precision: 0.762338 - Validation Recall: 0.741017 - Validation F1: 0.746263\n",
      "108/108 [==============================] - 11459s 107s/step - loss: 0.5509 - accuracy: 0.7557 - val_loss: 0.5257 - val_accuracy: 0.7622\n",
      "Epoch 3/10\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4948 - accuracy: 0.7766 \n",
      "Epoch: 3 - Training Precision: 0.786197 - Training Recall: 0.763083 - Training F1: 0.769758\n",
      "Epoch: 3 - Validation Precision: 0.770944 - Validation Recall: 0.751022 - Validation F1: 0.756305\n",
      "108/108 [==============================] - 8946s 83s/step - loss: 0.4948 - accuracy: 0.7766 - val_loss: 0.4923 - val_accuracy: 0.7709\n",
      "Epoch 4/10\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4634 - accuracy: 0.7909 \n",
      "Epoch: 4 - Training Precision: 0.799168 - Training Recall: 0.777066 - Training F1: 0.783819\n",
      "Epoch: 4 - Validation Precision: 0.784811 - Validation Recall: 0.766823 - Validation F1: 0.77208\n",
      "108/108 [==============================] - 4742s 44s/step - loss: 0.4634 - accuracy: 0.7909 - val_loss: 0.4721 - val_accuracy: 0.7848\n",
      "Epoch 5/10\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4408 - accuracy: 0.8019 \n",
      "Epoch: 5 - Training Precision: 0.816698 - Training Recall: 0.788475 - Training F1: 0.796598\n",
      "Epoch: 5 - Validation Precision: 0.799008 - Validation Recall: 0.775991 - Validation F1: 0.782261\n",
      "108/108 [==============================] - 6355s 59s/step - loss: 0.4408 - accuracy: 0.8019 - val_loss: 0.4599 - val_accuracy: 0.7956\n",
      "Epoch 6/10\n",
      " 20/108 [====>.........................] - ETA: 26:53 - loss: 0.4190 - accuracy: 0.8125"
     ]
    }
   ],
   "source": [
    "# Training, Evaluation, and Prediction:\n",
    "clf = DisasterDetector(bert_layer, max_seq_length=128, lr=0.0001, epochs=10, batch_size=32)\n",
    "clf.train(df_train_cleanedMislabelsNoDuplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b30109-94eb-484b-a98a-570f19886bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.plot_learning_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc2ccee-8fb4-4557-bbee-30539ebeb9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(df_test)\n",
    "\n",
    "model_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n",
    "model_submission['target'] = np.round(y_pred).astype('int')\n",
    "model_submission.to_csv('model_submission.csv', index=False)\n",
    "model_submission.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f0570c-0e51-45af-a54c-b0d3edffc57a",
   "metadata": {},
   "source": [
    "#### Building Vectors:\n",
    "We use sckit-learn's CountVectorizer to count the words in each tweet and turn them into data our machine learning model can process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f675506b-bd3f-41c9-8036-e25509efb83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "\n",
    "train_vectors = count_vectorizer.fit_transform(df_train_cleanedNoMislabelsNoDuplicates['text'])\n",
    "\n",
    "#For test_vectors, we only use .transform() function b/c we need that the train and test vectors use the same set of tokens\n",
    "test_vectors = count_vectorizer.transform(df_test_cleaned[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0256181b-fff7-48db-a63e-f6079a57c6b2",
   "metadata": {},
   "source": [
    "#### Building Model:\n",
    "We use the words contained in each tweet as the predictor variable for real/fake disaster tweet (1/0). We assume a linear model/connection in this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4549f19-a372-4e8c-a764-61e0a4b960c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [0.5767098  0.53946621 0.5949214 ]\n"
     ]
    }
   ],
   "source": [
    "# Since the vectors are big, we want to push the model's weights toward 0 without completely discounting different words\n",
    "# Thus, we use Ridge Regression\n",
    "clf = linear_model.RidgeClassifier()\n",
    "# cv = 3 means we are using Three-Fold Cross Validation\n",
    "scores = model_selection.cross_val_score(clf, train_vectors, df_train_cleanedNoMislabelsNoDuplicates['target_relabeled'], cv = 3, scoring = \"f1\")\n",
    "print(\"Scores: \", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32e33e1-8576-4ad0-8e39-84e2e0ce3146",
   "metadata": {},
   "source": [
    "Ways to improve this score: Do TFIDF, LSA, LSTM/RNNs, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623320ee-30ed-47a4-a245-58ef7f6b3f9c",
   "metadata": {},
   "source": [
    "#### Predictions and Submission File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dc04fd8-a4c1-4d3c-9729-2ae95a548153",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(train_vectors, train_df['target'])\n",
    "tutorial_submission = pd.read_csv(\"../Data/sample_submission.csv\")\n",
    "tutorial_submission['target'] = clf.predict(test_vectors)\n",
    "tutorial_submission.to_csv(\"../Result Files/submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50db6d7a-e2c3-4bb6-bbd4-38b6f0e95410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
